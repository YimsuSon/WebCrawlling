{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "section02_Basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPegQzEMhrygc6ufyPQ1R2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YimsuSon/WebCrawlling/blob/main/section02_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaXR_Il0Y2tg",
        "outputId": "ad96fdf5-a7d1-4abb-b5ae-af9f74bf0977"
      },
      "source": [
        "pip install lxml\n",
        "pip install requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqsPmMJZY69v",
        "outputId": "02a8e87b-00cc-4548-bcc4-c088976ee6be"
      },
      "source": [
        "pip install cssselect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect\n",
            "Successfully installed cssselect-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-eHLQiz7vF3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSOj1mDs7u_c",
        "outputId": "07fd40e7-aacc-439b-91b4-f4fec7a08385"
      },
      "source": [
        "# section02-1 \n",
        "# 기본 이미지 다운로드 \n",
        "import urllib.request as req\n",
        "\n",
        "\n",
        "# 파일 url\n",
        "img_url = 'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMTA2MDhfMjY5%2FMDAxNjIzMTYxNjAwNTg2.1TcjW1kOKIAsiCh4eko9-FD-UeHzqq598iHQ4WZybjgg.MnRgVxOiXtsq3q9V2dordct3lC5fwq4FNZ5eti16hewg.JPEG.yoon5172%2FKakaoTalk_20210608_060042648_03.jpg&type=sc960_832'\n",
        "html_url = 'https://google.com'\n",
        "\n",
        "\n",
        "# 다운 경로 \n",
        "save_path1 = 'test1.jpg'\n",
        "save_path2 = 'index.html'\n",
        "\n",
        "# 예외처리 \n",
        "\n",
        "try:\n",
        "    file1, header1 = req.urlretrieve(img_url,save_path1) ## 리턴 두개 = 파일 이름, header값\n",
        "    file2, header2 = req.urlretrieve(html_url,save_path2)\n",
        "except Exception as e:\n",
        "    print('download faild')\n",
        "    print(e)\n",
        "else:\n",
        "    # Header 정보 \n",
        "    print(header1)\n",
        "    print(header2)\n",
        "\n",
        "    # 다운로드 파일 정보\n",
        "    print('Filename1 {}'.format(file1))\n",
        "    print('Filename2 {}'.format(file2))\n",
        "    print()\n",
        "\n",
        "    # 성공 \n",
        "    print('성공')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accept-Ranges: bytes\n",
            "Content-Length: 131285\n",
            "Content-Type: image/jpeg\n",
            "Last-Modified: Sat, 12 Jun 2021 04:59:03 GMT\n",
            "p3p: CP=\"ALL CURa ADMa DEVa TAIa OUR BUS IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE LOC OTC\"\n",
            "Server: Testa/5.1.1\n",
            "Cache-Control: max-age=1629777\n",
            "Expires: Mon, 12 Jul 2021 04:58:21 GMT\n",
            "Date: Wed, 23 Jun 2021 08:15:24 GMT\n",
            "Connection: close\n",
            "\n",
            "\n",
            "Date: Wed, 23 Jun 2021 08:15:24 GMT\n",
            "Expires: -1\n",
            "Cache-Control: private, max-age=0\n",
            "Content-Type: text/html; charset=ISO-8859-1\n",
            "P3P: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\n",
            "Server: gws\n",
            "X-XSS-Protection: 0\n",
            "X-Frame-Options: SAMEORIGIN\n",
            "Set-Cookie: 1P_JAR=2021-06-23-08; expires=Fri, 23-Jul-2021 08:15:24 GMT; path=/; domain=.google.com; Secure\n",
            "Set-Cookie: NID=217=K9JU-3oEovwG0InvErURqkmxSMg8ZrPWj2i2WPJ7fZP6KHqZrKSQYm5IXSI5_Z0HMlZT0rJGh7as0HVMf56rEsw0sv3Z1o4tL8Sinb2UWQMjBUGkY7wAzfbhRLNlKYhTWU70p22iPlAvaegcmYOiLvKUMku19QaJVYYNnYkVCIc; expires=Thu, 23-Dec-2021 08:15:24 GMT; path=/; domain=.google.com; HttpOnly\n",
            "Accept-Ranges: none\n",
            "Vary: Accept-Encoding\n",
            "Connection: close\n",
            "Transfer-Encoding: chunked\n",
            "\n",
            "\n",
            "Filename1 test1.jpg\n",
            "Filename2 index.html\n",
            "\n",
            "성공\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efTd4RGc7u7T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yafs592w7u34"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvbjsv3k7ux6"
      },
      "source": [
        "# Section02-2\n",
        "# 파이썬 크롤링 기초\n",
        "# urlopen 함수 기초 사용법 \n",
        "import urllib.request as req \n",
        "from urllib.error import URLError, HTTPError \n",
        "\n",
        "\n",
        "# 다운로드 경로 및 파일명 \n",
        "path_list = ['/Users/mac/Desktop/python_crwalling/02/test2.jpg' ,'/Users/mac/Desktop/python_crwalling/02/index2.html'  ]\n",
        "\n",
        "# 다운로드 리소스 url\n",
        "target_url = [\"https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2F20151208_41%2Ffhdnf80_1449563377692PooJc_JPEG%2F%25B5%25B5%25B0%25EE%25B5%25BF_%25C5%25B8%25BF%25F6%25C6%25D3%25B8%25AE%25BD%25BA1.jpg&type=a340\"\n",
        ",\"https://google.com\"]\n",
        "\n",
        "for i,url in enumerate(target_url): \n",
        "\n",
        "    try:\n",
        "        # 웹 수신 정보 읽기 \n",
        "        response = req.urlopen(url)\n",
        "\n",
        "        # 수신 내용 \n",
        "        contents = response.read()\n",
        "\n",
        "        print(\"----------------------\")\n",
        "\n",
        "        # 상태정보 중간 출력 \n",
        "        print(\"Header Info -{} : {} \".format(i,response.info()))\n",
        "        print(\"HTTP Status Code : {} \".format(response.getcode()))\n",
        "        print()\n",
        "\n",
        "        print(\"----------------------\")\n",
        "\n",
        "        # with 문을 사용해서 write 후 닫기 \n",
        "        with open(path_list[i],'wb') as c:\n",
        "            c.write(contents)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    except HTTPError as e:\n",
        "        print(\"Download failed.\")\n",
        "        print(\"HTTP Error code : \", e.code)\n",
        "    except URLError as e:\n",
        "        print(\"Downdload failed\")\n",
        "        print(\"URL Error Reason : \", e.reason)\n",
        "    # 성공 \n",
        "    else:\n",
        "        print()\n",
        "        print(\"Download successed.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8QZdWya7uu5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_l2boxl7uo8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TQHl8KzY9_e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1G04zbmZHRT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ou3R4hDZI1a",
        "outputId": "02f34aa3-a79b-421f-a542-e1d32ab9110a"
      },
      "source": [
        "# Section02-3 \n",
        "# lxml 사용 기초 스크래핑(1)\n",
        "\n",
        "import lxml.html\n",
        "import requests \n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    네이버 메인 뉴스 \n",
        "    \"\"\"\n",
        "    # 스크랩핑 대상 URL\n",
        "    response = requests.get(\"https://news.daum.net/\")\n",
        "\n",
        "\n",
        "    # 신문사 링크 리스트 획득 \n",
        "    urls = scrape_news_list_page(response)\n",
        "\n",
        "\n",
        "    # 결과 출력 \n",
        "    for url in urls: \n",
        "        print(url)\n",
        "        # 파일 쓰기 \n",
        "        # 생략 \n",
        "\n",
        "def scrape_news_list_page(response):\n",
        "    # URL 리스트 선언 \n",
        "    urls = []\n",
        "\n",
        "    # 태그 정보 문자열 저장\n",
        "    \n",
        "    root = lxml.html.fromstring(response.content)\n",
        "    # ul , class명  ,class명\n",
        "    for a in root.cssselect('.list_headline .tit_g .link_txt'):\n",
        "      # 링크 \n",
        "      url = a.get('href') \n",
        "      urls.append(url)\n",
        "    return urls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 스크랩핑 시작 \n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://v.daum.net/v/20210623111754717\n",
            "https://v.daum.net/v/20210623115618257\n",
            "https://v.daum.net/v/20210623114831026\n",
            "https://v.daum.net/v/20210623134513269\n",
            "https://v.daum.net/v/20210623151045930\n",
            "https://v.daum.net/v/20210623144631800\n",
            "https://v.daum.net/v/20210623130133945\n",
            "https://v.daum.net/v/20210623142852971\n",
            "https://v.daum.net/v/20210623133015701\n",
            "https://v.daum.net/v/20210623145056988\n",
            "https://v.daum.net/v/20210623112111881\n",
            "https://v.daum.net/v/20210623132658608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcUc18W_ZfaX"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ts7hciijE2"
      },
      "source": [
        "# section 02-4 \n",
        "# lxml 사용 기초 스크랩핑(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwU0eVLujU66"
      },
      "source": [
        "from lxml.html import fromstring, tostring"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tauBDO0riorD",
        "outputId": "0959af6a-02fe-465f-9cc6-77dfa7a3e077"
      },
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    네이버 메인 뉴스 \n",
        "    \"\"\"\n",
        "  # 세션 사용 \n",
        "    session = requests.Session()\n",
        "\n",
        "\n",
        "      # 스크랩핑 대상 URL\n",
        "    response = session.get(\"https://news.daum.net/\")\n",
        "\n",
        "\n",
        "    # 신문사 링크 리스트 획득 \n",
        "    urls = scrape_news_list_page(response)\n",
        "\n",
        "    # 딕셔너리 확인 \n",
        "    # print(urls)\n",
        "\n",
        "\n",
        "    # # 결과 출력 \n",
        "    # for name,url in urls.items(): \n",
        "    #     print(name,url)\n",
        "    #     # 파일 쓰기 \n",
        "    #     # 생략 \n",
        "\n",
        "def scrape_news_list_page(response):\n",
        "    # URL 딕셔너리 선언 \n",
        "    urls = {}\n",
        "\n",
        "    # 태그 정보 문자열 저장\n",
        "    \n",
        "    root = fromstring(response.content)\n",
        "    # ul , class명  ,class명\n",
        "    for a in root.xpath('//ul[@class=\"list_headline\"]/li/strong[@class=\"tit_g\"]/span[@class=\"info_news\"]'):\n",
        "    #for a in root.xpath('//ul[@class=\"list_headline\"]/li/strong[@class=\"tit_g\"]'):       \n",
        "      # 구조 확인 \n",
        "        #  print(a)\n",
        "\n",
        "      # 문자열 출력\n",
        "        print(tostring(a, pretty_print = True) )\n",
        "\n",
        "#         name, url = extract_contents(a)\n",
        "#         #딕셔너리 삽입 \n",
        "#         urls[name] = url\n",
        " \n",
        "#     return urls \n",
        "\n",
        "\n",
        "# def extract_contents(dom):\n",
        "\n",
        "#   # 링크 주소 \n",
        "#   link = dom.get(\"href\")\n",
        "\n",
        "#   # 신문사 명 \n",
        "#   name = \"a\" #dom.xpath('./strong[@class=\"info_news\"]')\n",
        "#   print(name)\n",
        "\n",
        "#   return name, link \n",
        "\n",
        "\n",
        "\n",
        "# 스크랩핑 시작 \n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'<span class=\"info_news\">&#50672;&#54633;&#45684;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#49436;&#50872;&#49888;&#47928;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#45684;&#49884;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#50672;&#54633;&#45684;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#45684;&#49828;1</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#45684;&#49884;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#50672;&#54633;&#45684;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#46041;&#50500;&#51068;&#48372;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#45684;&#49884;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#50672;&#54633;&#45684;&#49828;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#49464;&#44228;&#51068;&#48372;</span>\\n\\t\\t\\t            \\n'\n",
            "b'<span class=\"info_news\">&#45684;&#49884;&#49828;</span>\\n\\t\\t\\t            \\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkSIRxWSjcQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq6DgOTL_Meb",
        "outputId": "f20dc330-b409-4511-d272-6423b78a1abc"
      },
      "source": [
        "!git clone https://github.com/YimsuSon/WebCrawlling.git"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'WebCrawlling2'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCi2O7s1_SGu",
        "outputId": "525154b3-e3c2-405e-e6ec-419df78e0e93"
      },
      "source": [
        "ls -ltr"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 156\n",
            "drwxr-xr-x 1 root root   4096 Jun 15 13:37 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root 131285 Jun 23 08:15 test1.jpg\n",
            "-rw-r--r-- 1 root root  12956 Jun 23 08:15 index.html\n",
            "drwxr-xr-x 3 root root   4096 Jun 23 08:29 \u001b[01;34mWebCrawlling\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrZoozrn_09_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}